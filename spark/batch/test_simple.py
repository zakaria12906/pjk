#!/usr/bin/env python3
"""Test simple Spark - Lecture logs HDFS"""

from pyspark.sql import SparkSession
import re

# Cr√©er session Spark
spark = SparkSession.builder \
    .appName("Test Simple") \
    .getOrCreate()

print("=" * 50)
print("‚úÖ Spark Session cr√©√©e avec succ√®s")
print("=" * 50)

# Lire le fichier de logs
try:
    logs_rdd = spark.sparkContext.textFile("hdfs://namenode:9000/logs/web_server.log")
    print(f"‚úÖ Fichier charg√© depuis HDFS")
    print(f"‚úÖ Nombre de lignes: {logs_rdd.count()}")
    print("=" * 50)
    print("Premi√®res lignes:")
    for line in logs_rdd.take(5):
        print(line)
    print("=" * 50)
except Exception as e:
    print(f"‚ùå Erreur: {e}")

# Analyser les produits
def extract_product_id(line):
    """Extraire l'ID du produit"""
    match = re.search(r'/products/[^?]+\?id=(\d+)', line)
    if match:
        return match.group(1)
    return None

# Extraire les IDs
product_ids = logs_rdd.map(extract_product_id).filter(lambda x: x is not None)
print(f"‚úÖ Nombre de requ√™tes produits: {product_ids.count()}")

# Top 10 produits
from operator import add
top_products = product_ids.map(lambda x: (x, 1)) \
    .reduceByKey(add) \
    .sortBy(lambda x: x[1], ascending=False) \
    .take(10)

print("=" * 50)
print("üèÜ TOP 10 PRODUITS:")
for i, (product_id, count) in enumerate(top_products, 1):
    print(f"{i}. Produit ID {product_id}: {count} requ√™tes")
print("=" * 50)

spark.stop()
print("‚úÖ Test termin√© avec succ√®s!")
