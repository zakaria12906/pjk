#!/usr/bin/env python3
"""
Analyse Batch Spark #2: R√©partition des Codes HTTP
===================================================

Objectif:
    Analyser la fr√©quence des codes HTTP (200, 404, 500, etc.) pour
    √©valuer les performances et la sant√© du serveur web.

M√©thode:
    1. Charger les logs depuis HDFS
    2. Parser les lignes pour extraire les codes HTTP
    3. Compter les occurrences de chaque code
    4. Calculer les pourcentages
    5. Classifier les codes par cat√©gorie (succ√®s, erreur, redirection)
    6. Sauvegarder les r√©sultats dans MongoDB

KPIs g√©n√©r√©s:
    - Taux de succ√®s (2xx)
    - Taux d'erreur client (4xx)
    - Taux d'erreur serveur (5xx)
    - Taux de redirection (3xx)

Architecture:
    HDFS ‚Üí Spark (RDD/DataFrame) ‚Üí MongoDB
"""

from pyspark.sql import SparkSession
from pyspark.sql.functions import col, count, desc, sum as spark_sum, round as spark_round
import re

# Configuration MongoDB
MONGO_URI = "mongodb://mongodb:27017"
MONGO_DB = "logs_analytics"
MONGO_COLLECTION = "http_codes"

# Classification des codes HTTP
CODE_CATEGORIES = {
    "2xx - Succ√®s": [200, 201, 202, 204],
    "3xx - Redirection": [301, 302, 303, 304, 307, 308],
    "4xx - Erreur Client": [400, 401, 403, 404, 405, 408, 429],
    "5xx - Erreur Serveur": [500, 502, 503, 504]
}

def create_spark_session():
    """Cr√©e une session Spark avec configuration MongoDB"""
    spark = SparkSession.builder \
        .appName("HTTP Codes Analysis") \
        .config("spark.mongodb.output.uri", f"{MONGO_URI}/{MONGO_DB}.{MONGO_COLLECTION}") \
        .getOrCreate()
    
    return spark

def parse_http_code(line):
    """
    Parse une ligne de log et extrait le code HTTP
    
    Format: IP - - [timestamp] "METHOD URL HTTP/1.1" CODE SIZE
    Exemple: 192.168.1.100 - - [28/Jan/2025:15:40:02 +0000] "GET /products/lipstick?id=105 HTTP/1.1" 301 512
    """
    # Regex pour extraire le code HTTP (apr√®s le dernier guillemet)
    pattern = r'"\s+(\d{3})\s+'
    match = re.search(pattern, line)
    if match:
        return int(match.group(1))
    return None

def categorize_code(code):
    """
    Cat√©gorise un code HTTP selon sa classe
    
    Args:
        code: Code HTTP (int)
    
    Returns:
        Cat√©gorie du code (str)
    """
    for category, codes in CODE_CATEGORIES.items():
        if code in codes:
            return category
    return "Autre"

def analyze_http_codes(spark, input_path):
    """
    Analyse principale: r√©partition des codes HTTP
    
    Args:
        spark: Session Spark
        input_path: Chemin HDFS des logs
    
    Returns:
        Tuple (codes_df, categories_df) avec les DataFrames de r√©sultats
    """
    print("\n" + "="*60)
    print("üöÄ D√âMARRAGE DE L'ANALYSE: CODES HTTP")
    print("="*60)
    
    # √âtape 1: Charger les donn√©es depuis HDFS
    print("\nüìÇ Chargement des logs depuis HDFS...")
    logs_rdd = spark.sparkContext.textFile(input_path)
    total_lines = logs_rdd.count()
    print(f"   ‚úì {total_lines} lignes charg√©es")
    
    # √âtape 2: Parser les codes HTTP
    print("\nüîç Extraction des codes HTTP...")
    http_codes_rdd = logs_rdd.map(parse_http_code).filter(lambda x: x is not None)
    valid_codes = http_codes_rdd.count()
    print(f"   ‚úì {valid_codes} codes HTTP extraits")
    
    # √âtape 3: Compter les occurrences par code
    print("\nüìä Comptage des occurrences par code...")
    code_counts = http_codes_rdd.map(lambda x: (x, 1)) \
                                 .reduceByKey(lambda a, b: a + b)
    
    # Convertir en DataFrame
    codes_df = code_counts.toDF(["http_code", "count"])
    
    # Calculer les pourcentages
    total_requests = codes_df.agg(spark_sum("count")).collect()[0][0]
    codes_df = codes_df.withColumn("percentage", 
                                   spark_round((col("count") / total_requests) * 100, 2))
    
    # Trier par nombre de requ√™tes
    codes_df = codes_df.orderBy(desc("count"))
    
    # Affichage des r√©sultats d√©taill√©s
    print("\n" + "="*60)
    print("üìà R√âSULTATS: R√âPARTITION DES CODES HTTP")
    print("="*60)
    codes_df.show(truncate=False)
    
    # √âtape 4: Analyse par cat√©gorie
    print("\nüìä Analyse par cat√©gorie...")
    
    # Ajouter la cat√©gorie √† chaque code
    from pyspark.sql.functions import udf
    from pyspark.sql.types import StringType
    
    categorize_udf = udf(categorize_code, StringType())
    codes_with_category = codes_df.withColumn("category", categorize_udf(col("http_code")))
    
    # Agr√©ger par cat√©gorie
    categories_df = codes_with_category.groupBy("category") \
                                       .agg(spark_sum("count").alias("count"),
                                            spark_sum("percentage").alias("percentage"))
    
    categories_df = categories_df.orderBy(desc("count"))
    
    print("\n" + "="*60)
    print("üìä R√âPARTITION PAR CAT√âGORIE")
    print("="*60)
    categories_df.show(truncate=False)
    
    # √âtape 5: KPIs de sant√© du serveur
    print("\n" + "="*60)
    print("üè• INDICATEURS DE SANT√â DU SERVEUR (KPIs)")
    print("="*60)
    
    categories_list = categories_df.collect()
    kpis = {}
    
    for row in categories_list:
        category = row["category"]
        percentage = row["percentage"]
        
        if "Succ√®s" in category:
            kpis["success_rate"] = percentage
            print(f"   ‚úÖ Taux de succ√®s (2xx): {percentage:.2f}%")
        elif "Redirection" in category:
            kpis["redirect_rate"] = percentage
            print(f"   üîÑ Taux de redirection (3xx): {percentage:.2f}%")
        elif "Erreur Client" in category:
            kpis["client_error_rate"] = percentage
            print(f"   ‚ö†Ô∏è  Taux d'erreur client (4xx): {percentage:.2f}%")
        elif "Erreur Serveur" in category:
            kpis["server_error_rate"] = percentage
            print(f"   ‚ùå Taux d'erreur serveur (5xx): {percentage:.2f}%")
    
    # √âvaluation de la sant√©
    print("\nüí° √âvaluation:")
    success_rate = kpis.get("success_rate", 0)
    server_error_rate = kpis.get("server_error_rate", 0)
    
    if success_rate >= 95 and server_error_rate < 1:
        print("   üü¢ Serveur en EXCELLENTE sant√©")
    elif success_rate >= 85 and server_error_rate < 3:
        print("   üü° Serveur en BONNE sant√©")
    elif success_rate >= 70:
        print("   üü† Serveur en sant√© MOYENNE - Attention requise")
    else:
        print("   üî¥ Serveur en MAUVAISE sant√© - Action imm√©diate requise!")
    
    # Ajouter les KPIs au DataFrame des cat√©gories
    from pyspark.sql.functions import lit, current_timestamp
    categories_df = categories_df.withColumn("analyzed_at", current_timestamp())
    
    return codes_df, categories_df, kpis

def save_to_mongodb(codes_df, categories_df, kpis):
    """
    Sauvegarde les r√©sultats dans MongoDB
    
    Args:
        codes_df: DataFrame avec les codes d√©taill√©s
        categories_df: DataFrame avec les cat√©gories
        kpis: Dictionnaire des KPIs
    """
    print(f"\nüíæ Sauvegarde des r√©sultats dans MongoDB...")
    
    from pyspark.sql.functions import current_timestamp
    
    # Sauvegarder les codes d√©taill√©s
    codes_df_timestamped = codes_df.withColumn("analyzed_at", current_timestamp())
    codes_df_timestamped.write \
        .format("mongo") \
        .mode("overwrite") \
        .option("database", MONGO_DB) \
        .option("collection", "http_codes_detailed") \
        .save()
    print("   ‚úì Codes d√©taill√©s sauvegard√©s")
    
    # Sauvegarder les cat√©gories
    categories_df.write \
        .format("mongo") \
        .mode("overwrite") \
        .option("database", MONGO_DB) \
        .option("collection", "http_codes_categories") \
        .save()
    print("   ‚úì Cat√©gories sauvegard√©es")
    
    # Sauvegarder les KPIs dans une collection s√©par√©e
    from datetime import datetime
    kpis_data = [{
        "analyzed_at": datetime.now(),
        "success_rate": kpis.get("success_rate", 0),
        "redirect_rate": kpis.get("redirect_rate", 0),
        "client_error_rate": kpis.get("client_error_rate", 0),
        "server_error_rate": kpis.get("server_error_rate", 0)
    }]
    
    spark = SparkSession.getActiveSession()
    kpis_df = spark.createDataFrame(kpis_data)
    kpis_df.write \
        .format("mongo") \
        .mode("overwrite") \
        .option("database", MONGO_DB) \
        .option("collection", "server_health_kpis") \
        .save()
    print("   ‚úì KPIs de sant√© sauvegard√©s")
    
    print(f"\n   ‚ÑπÔ∏è  Requ√™tes MongoDB:")
    print(f"      db.http_codes_detailed.find().pretty()")
    print(f"      db.http_codes_categories.find().pretty()")
    print(f"      db.server_health_kpis.find().pretty()")

def main():
    """Fonction principale"""
    # Chemins
    INPUT_PATH = "hdfs://namenode:9000/logs/web_server.log"
    
    # Cr√©er la session Spark
    spark = create_spark_session()
    spark.sparkContext.setLogLevel("WARN")
    
    try:
        # Ex√©cuter l'analyse
        codes_df, categories_df, kpis = analyze_http_codes(spark, INPUT_PATH)
        
        # Sauvegarder dans MongoDB
        save_to_mongodb(codes_df, categories_df, kpis)
        
        print("\n" + "="*60)
        print("‚úÖ ANALYSE TERMIN√âE AVEC SUCC√àS!")
        print("="*60 + "\n")
        
    except Exception as e:
        print(f"\n‚ùå ERREUR: {str(e)}")
        import traceback
        traceback.print_exc()
    finally:
        spark.stop()

if __name__ == "__main__":
    main()
