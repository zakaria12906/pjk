#!/usr/bin/env python3
"""
Analyse Streaming Spark #1: D√©tection d'Erreurs en Temps R√©el
==============================================================

Objectif:
    Surveiller les logs en temps r√©el pour d√©tecter des pics d'erreurs
    (codes 404 ou 500) et g√©n√©rer des alertes.

M√©thode:
    1. Consommer les logs depuis Kafka en temps r√©el
    2. Parser les lignes pour extraire IP, URL, et code HTTP
    3. Filtrer les erreurs (404, 500)
    4. Fen√™trage temporel (5 minutes)
    5. Compter les erreurs par type dans chaque fen√™tre
    6. G√©n√©rer des alertes si seuil d√©pass√© (> 10 erreurs)
    7. Sauvegarder les alertes dans MongoDB

Fen√™trage:
    - Window size: 5 minutes
    - Slide interval: 1 minute
    - Watermark: 30 secondes (pour g√©rer les donn√©es en retard)

Alertes:
    - CRITIQUE: > 20 erreurs 500 en 5 minutes
    - HAUTE: > 10 erreurs 500 en 5 minutes
    - MOYENNE: > 30 erreurs 404 en 5 minutes

Architecture:
    Kafka ‚Üí Spark Structured Streaming ‚Üí MongoDB
"""

from pyspark.sql import SparkSession
from pyspark.sql.functions import (
    from_json, col, window, count, current_timestamp, 
    lit, when, sum as spark_sum
)
from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType
import re

# Configuration
KAFKA_BOOTSTRAP_SERVERS = "kafka:9093"
KAFKA_TOPIC = "web-logs"
MONGO_URI = "mongodb://mongodb:27017"
MONGO_DB = "logs_analytics"
MONGO_COLLECTION = "error_alerts"

# Seuils d'alerte
CRITICAL_500_THRESHOLD = 20  # Erreurs 500 critiques
HIGH_500_THRESHOLD = 10      # Erreurs 500 haute priorit√©
MEDIUM_404_THRESHOLD = 30    # Erreurs 404 moyenne priorit√©

def create_spark_session():
    """Cr√©e une session Spark Structured Streaming"""
    spark = SparkSession.builder \
        .appName("Real-Time Error Detection") \
        .config("spark.mongodb.output.uri", f"{MONGO_URI}/{MONGO_DB}.{MONGO_COLLECTION}") \
        .config("spark.streaming.stopGracefullyOnShutdown", "true") \
        .getOrCreate()
    
    return spark

def parse_log_udf(log_line):
    """
    Parse une ligne de log et retourne un dictionnaire
    
    Format: IP - - [timestamp] "METHOD URL HTTP/1.1" CODE SIZE
    """
    try:
        # Extraire l'IP
        ip_pattern = r'^([\d\.]+)'
        ip_match = re.match(ip_pattern, log_line)
        
        # Extraire le timestamp
        timestamp_pattern = r'\[(.*?)\]'
        timestamp_match = re.search(timestamp_pattern, log_line)
        
        # Extraire l'URL
        url_pattern = r'\"[A-Z]+\s+([^\s]+)\s+HTTP'
        url_match = re.search(url_pattern, log_line)
        
        # Extraire le code HTTP
        code_pattern = r'"\s+(\d{3})\s+'
        code_match = re.search(code_pattern, log_line)
        
        if ip_match and timestamp_match and url_match and code_match:
            return {
                "ip": ip_match.group(1),
                "timestamp": timestamp_match.group(1),
                "url": url_match.group(1),
                "http_code": int(code_match.group(1))
            }
    except Exception as e:
        print(f"Erreur parsing: {e}")
    
    return None

def setup_streaming_query(spark):
    """
    Configure la requ√™te de streaming Spark
    
    Returns:
        StreamingQuery object
    """
    print("\n" + "="*60)
    print("üöÄ D√âMARRAGE DU STREAMING: D√âTECTION D'ERREURS")
    print("="*60)
    
    # √âtape 1: Lire depuis Kafka
    print("\nüì° Connexion √† Kafka...")
    print(f"   ‚Ä¢ Topic: {KAFKA_TOPIC}")
    print(f"   ‚Ä¢ Bootstrap servers: {KAFKA_BOOTSTRAP_SERVERS}")
    
    kafka_df = spark.readStream \
        .format("kafka") \
        .option("kafka.bootstrap.servers", KAFKA_BOOTSTRAP_SERVERS) \
        .option("subscribe", KAFKA_TOPIC) \
        .option("startingOffsets", "latest") \
        .load()
    
    # √âtape 2: Convertir les valeurs Kafka en string
    logs_df = kafka_df.selectExpr("CAST(value AS STRING) as log_line", "timestamp")
    
    # √âtape 3: Parser les logs avec regex
    from pyspark.sql.functions import regexp_extract
    
    parsed_df = logs_df.select(
        regexp_extract(col("log_line"), r'^([\d\.]+)', 1).alias("ip"),
        col("timestamp").alias("event_time"),
        regexp_extract(col("log_line"), r'\"[A-Z]+\s+([^\s]+)\s+HTTP', 1).alias("url"),
        regexp_extract(col("log_line"), r'"\s+(\d{3})\s+', 1).cast("int").alias("http_code")
    ).filter(col("http_code").isNotNull())
    
    # √âtape 4: Filtrer uniquement les erreurs (4xx et 5xx)
    error_df = parsed_df.filter(
        (col("http_code") >= 400) & (col("http_code") < 600)
    )
    
    # √âtape 5: Classifier les erreurs
    error_df = error_df.withColumn(
        "error_type",
        when(col("http_code") == 404, "NOT_FOUND")
        .when(col("http_code") == 500, "INTERNAL_ERROR")
        .when(col("http_code") == 503, "SERVICE_UNAVAILABLE")
        .when(col("http_code") == 403, "FORBIDDEN")
        .otherwise("OTHER_ERROR")
    )
    
    # √âtape 6: Fen√™trage temporel (5 minutes, slide 1 minute)
    print("\n‚è±Ô∏è  Configuration du fen√™trage temporel:")
    print("   ‚Ä¢ Taille de fen√™tre: 5 minutes")
    print("   ‚Ä¢ Intervalle de glissement: 1 minute")
    print("   ‚Ä¢ Watermark: 30 secondes")
    
    # Ajouter un watermark pour g√©rer les donn√©es en retard
    error_df = error_df.withWatermark("event_time", "30 seconds")
    
    # Agr√©ger les erreurs par fen√™tre et type
    windowed_errors = error_df.groupBy(
        window(col("event_time"), "5 minutes", "1 minute"),
        col("error_type"),
        col("http_code")
    ).agg(
        count("*").alias("error_count"),
        count(col("ip")).alias("unique_ips")
    )
    
    # √âtape 7: G√©n√©rer des alertes bas√©es sur les seuils
    print("\nüö® Configuration des seuils d'alerte:")
    print(f"   ‚Ä¢ CRITIQUE (500): > {CRITICAL_500_THRESHOLD} erreurs")
    print(f"   ‚Ä¢ HAUTE (500): > {HIGH_500_THRESHOLD} erreurs")
    print(f"   ‚Ä¢ MOYENNE (404): > {MEDIUM_404_THRESHOLD} erreurs")
    
    alerts_df = windowed_errors.withColumn(
        "alert_level",
        when(
            (col("http_code") == 500) & (col("error_count") > CRITICAL_500_THRESHOLD),
            "CRITICAL"
        )
        .when(
            (col("http_code") == 500) & (col("error_count") > HIGH_500_THRESHOLD),
            "HIGH"
        )
        .when(
            (col("http_code") == 404) & (col("error_count") > MEDIUM_404_THRESHOLD),
            "MEDIUM"
        )
        .otherwise("INFO")
    )
    
    alerts_df = alerts_df.withColumn(
        "alert_message",
        when(col("alert_level") == "CRITICAL",
             lit("üî¥ ALERTE CRITIQUE: Pic d'erreurs 500 d√©tect√©!"))
        .when(col("alert_level") == "HIGH",
             lit("üü† ALERTE HAUTE: Erreurs 500 anormales!"))
        .when(col("alert_level") == "MEDIUM",
             lit("üü° ALERTE MOYENNE: Nombreuses erreurs 404!"))
        .otherwise(lit("‚ÑπÔ∏è INFO: Erreurs normales"))
    )
    
    # Ajouter timestamp de d√©tection
    alerts_df = alerts_df.withColumn("detected_at", current_timestamp())
    
    # S√©lectionner les colonnes finales
    final_df = alerts_df.select(
        col("window.start").alias("window_start"),
        col("window.end").alias("window_end"),
        col("error_type"),
        col("http_code"),
        col("error_count"),
        col("unique_ips"),
        col("alert_level"),
        col("alert_message"),
        col("detected_at")
    )
    
    return final_df

def write_to_mongodb(df):
    """
    Configure l'√©criture vers MongoDB
    
    Args:
        df: DataFrame √† √©crire
    
    Returns:
        StreamingQuery
    """
    print("\nüíæ Configuration de l'√©criture vers MongoDB...")
    
    query = df.writeStream \
        .outputMode("append") \
        .format("mongo") \
        .option("database", MONGO_DB) \
        .option("collection", MONGO_COLLECTION) \
        .option("checkpointLocation", "/tmp/spark-checkpoint-errors") \
        .start()
    
    return query

def write_to_console(df):
    """
    Configuration de l'affichage console (pour debugging)
    
    Args:
        df: DataFrame √† afficher
    
    Returns:
        StreamingQuery
    """
    print("\nüì∫ Configuration de l'affichage console...")
    
    query = df.writeStream \
        .outputMode("append") \
        .format("console") \
        .option("truncate", "false") \
        .start()
    
    return query

def main():
    """Fonction principale"""
    # Cr√©er la session Spark
    spark = create_spark_session()
    spark.sparkContext.setLogLevel("WARN")
    
    try:
        # Setup streaming
        alerts_df = setup_streaming_query(spark)
        
        # √âcriture simultan√©e vers MongoDB et console
        mongodb_query = write_to_mongodb(alerts_df)
        console_query = write_to_console(alerts_df)
        
        print("\n" + "="*60)
        print("‚úÖ STREAMING D√âMARR√â - EN ATTENTE DE LOGS...")
        print("="*60)
        print("\nüí° Le syst√®me surveille maintenant les erreurs en temps r√©el")
        print("   Appuyez sur Ctrl+C pour arr√™ter\n")
        
        # Attendre la terminaison
        mongodb_query.awaitTermination()
        console_query.awaitTermination()
        
    except KeyboardInterrupt:
        print("\n\nüõë Arr√™t demand√© par l'utilisateur...")
        spark.streams.active[0].stop()
        print("‚úÖ Streaming arr√™t√© proprement\n")
    except Exception as e:
        print(f"\n‚ùå ERREUR: {str(e)}")
        import traceback
        traceback.print_exc()
    finally:
        spark.stop()

if __name__ == "__main__":
    main()
