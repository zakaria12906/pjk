#!/usr/bin/env python3
"""
Analyse Streaming Spark #2: DÃ©tection des Produits en Tendance
===============================================================

Objectif:
    Identifier en temps rÃ©el les produits populaires qui gÃ©nÃ¨rent
    un volume anormal de consultations (> 20 vues par minute).

MÃ©thode:
    1. Consommer les logs depuis Kafka en temps rÃ©el
    2. Parser et extraire les IDs de produits
    3. FenÃªtrage temporel (1 minute)
    4. Compter les consultations par produit
    5. DÃ©tecter les produits "en tendance" (> 20 vues/minute)
    6. Calculer la vÃ©locitÃ© (variation du nombre de vues)
    7. Sauvegarder les tendances dans MongoDB

FenÃªtrage:
    - Window size: 1 minute (dÃ©tection rapide)
    - Slide interval: 30 secondes (mise Ã  jour frÃ©quente)
    - Watermark: 20 secondes

CritÃ¨res de tendance:
    - HOT: > 50 consultations/minute
    - TRENDING: > 20 consultations/minute
    - RISING: 10-20 consultations/minute avec croissance

Architecture:
    Kafka â†’ Spark Structured Streaming â†’ MongoDB
"""

from pyspark.sql import SparkSession
from pyspark.sql.functions import (
    from_json, col, window, count, current_timestamp, 
    lit, when, regexp_extract, desc
)
from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType
import re

# Configuration
KAFKA_BOOTSTRAP_SERVERS = "kafka:9093"
KAFKA_TOPIC = "web-logs"
MONGO_URI = "mongodb://mongodb:27017"
MONGO_DB = "logs_analytics"
MONGO_COLLECTION = "trending_products"

# Seuils de tendance
HOT_THRESHOLD = 50        # Produit trÃ¨s populaire
TRENDING_THRESHOLD = 20   # Produit en tendance
RISING_THRESHOLD = 10     # Produit en croissance

def create_spark_session():
    """CrÃ©e une session Spark Structured Streaming"""
    spark = SparkSession.builder \
        .appName("Real-Time Trending Products") \
        .config("spark.mongodb.output.uri", f"{MONGO_URI}/{MONGO_DB}.{MONGO_COLLECTION}") \
        .config("spark.streaming.stopGracefullyOnShutdown", "true") \
        .getOrCreate()
    
    return spark

def extract_product_info(url):
    """
    Extrait l'ID et la catÃ©gorie du produit depuis l'URL
    
    Exemples:
        /products/lipstick?id=105 â†’ (105, "lipstick", "makeup")
        /products/skincare/cream?id=501 â†’ (501, "cream", "skincare")
    """
    if not url or '?id=' not in url:
        return None, None, None
    
    # Extraire l'ID
    id_match = re.search(r'\?id=(\d+)', url)
    if not id_match:
        return None, None, None
    
    product_id = int(id_match.group(1))
    
    # Extraire la catÃ©gorie et le type de produit
    category = "other"
    product_type = "unknown"
    
    if '/products/skincare/' in url:
        category = "skincare"
        product_type = url.split('/products/skincare/')[1].split('?')[0]
    elif '/products/hair/' in url:
        category = "hair"
        product_type = url.split('/products/hair/')[1].split('?')[0]
    elif '/products/' in url:
        category = "makeup"
        product_type = url.split('/products/')[1].split('?')[0]
    
    return product_id, product_type, category

def setup_streaming_query(spark):
    """
    Configure la requÃªte de streaming Spark
    
    Returns:
        DataFrame de streaming
    """
    print("\n" + "="*60)
    print("ğŸš€ DÃ‰MARRAGE DU STREAMING: PRODUITS EN TENDANCE")
    print("="*60)
    
    # Ã‰tape 1: Lire depuis Kafka
    print("\nğŸ“¡ Connexion Ã  Kafka...")
    print(f"   â€¢ Topic: {KAFKA_TOPIC}")
    print(f"   â€¢ Bootstrap servers: {KAFKA_BOOTSTRAP_SERVERS}")
    
    kafka_df = spark.readStream \
        .format("kafka") \
        .option("kafka.bootstrap.servers", KAFKA_BOOTSTRAP_SERVERS) \
        .option("subscribe", KAFKA_TOPIC) \
        .option("startingOffsets", "latest") \
        .load()
    
    # Ã‰tape 2: Convertir les valeurs Kafka en string
    logs_df = kafka_df.selectExpr("CAST(value AS STRING) as log_line", "timestamp")
    
    # Ã‰tape 3: Parser les logs pour extraire URL et code HTTP
    parsed_df = logs_df.select(
        regexp_extract(col("log_line"), r'^([\d\.]+)', 1).alias("ip"),
        col("timestamp").alias("event_time"),
        regexp_extract(col("log_line"), r'\"[A-Z]+\s+([^\s]+)\s+HTTP', 1).alias("url"),
        regexp_extract(col("log_line"), r'"\s+(\d{3})\s+', 1).cast("int").alias("http_code")
    ).filter(col("url").isNotNull())
    
    # Ã‰tape 4: Filtrer uniquement les URLs de produits (avec ?id=) et succÃ¨s (200)
    product_df = parsed_df.filter(
        (col("url").contains("?id=")) & (col("http_code") == 200)
    )
    
    # Ã‰tape 5: Extraire l'ID et la catÃ©gorie du produit
    product_df = product_df.select(
        col("event_time"),
        col("ip"),
        col("url"),
        regexp_extract(col("url"), r'\?id=(\d+)', 1).cast("int").alias("product_id"),
        when(col("url").contains("/skincare/"), "skincare")
        .when(col("url").contains("/hair/"), "hair")
        .otherwise("makeup").alias("category"),
        regexp_extract(
            col("url"),
            r'/products/(?:skincare/|hair/)?([^?]+)',
            1
        ).alias("product_type")
    ).filter(col("product_id").isNotNull())
    
    # Ã‰tape 6: Configuration du fenÃªtrage
    print("\nâ±ï¸  Configuration du fenÃªtrage temporel:")
    print("   â€¢ Taille de fenÃªtre: 1 minute")
    print("   â€¢ Intervalle de glissement: 30 secondes")
    print("   â€¢ Watermark: 20 secondes")
    
    # Ajouter watermark
    product_df = product_df.withWatermark("event_time", "20 seconds")
    
    # Ã‰tape 7: AgrÃ©ger par fenÃªtre et produit
    windowed_products = product_df.groupBy(
        window(col("event_time"), "1 minute", "30 seconds"),
        col("product_id"),
        col("product_type"),
        col("category")
    ).agg(
        count("*").alias("views_count"),
        count(col("ip").distinct()).alias("unique_viewers")
    )
    
    # Ã‰tape 8: Classifier les tendances
    print("\nğŸ“ˆ Configuration des seuils de tendance:")
    print(f"   â€¢ HOT: > {HOT_THRESHOLD} vues/minute")
    print(f"   â€¢ TRENDING: > {TRENDING_THRESHOLD} vues/minute")
    print(f"   â€¢ RISING: {RISING_THRESHOLD}-{TRENDING_THRESHOLD} vues/minute")
    
    trending_df = windowed_products.withColumn(
        "trend_status",
        when(col("views_count") > HOT_THRESHOLD, "HOT")
        .when(col("views_count") > TRENDING_THRESHOLD, "TRENDING")
        .when(col("views_count") >= RISING_THRESHOLD, "RISING")
        .otherwise("NORMAL")
    )
    
    # Ajouter des badges et messages
    trending_df = trending_df.withColumn(
        "trend_badge",
        when(col("trend_status") == "HOT", "ğŸ”¥")
        .when(col("trend_status") == "TRENDING", "ğŸ“ˆ")
        .when(col("trend_status") == "RISING", "â¬†ï¸")
        .otherwise("ğŸ“Š")
    )
    
    trending_df = trending_df.withColumn(
        "alert_message",
        when(col("trend_status") == "HOT",
             lit("ğŸ”¥ PRODUIT TRÃˆS POPULAIRE! VÃ©rifier le stock"))
        .when(col("trend_status") == "TRENDING",
             lit("ğŸ“ˆ PRODUIT EN TENDANCE! ConsidÃ©rer une promotion"))
        .when(col("trend_status") == "RISING",
             lit("â¬†ï¸ PRODUIT EN CROISSANCE! Surveiller de prÃ¨s"))
        .otherwise(lit("ğŸ“Š ActivitÃ© normale"))
    )
    
    # Calculer le ratio de viewers uniques (engagement)
    trending_df = trending_df.withColumn(
        "engagement_rate",
        (col("unique_viewers") / col("views_count") * 100).cast("int")
    )
    
    # Ajouter timestamp de dÃ©tection
    trending_df = trending_df.withColumn("detected_at", current_timestamp())
    
    # Filtrer pour ne garder que les produits intÃ©ressants (au moins RISING)
    trending_only = trending_df.filter(
        col("trend_status").isin(["HOT", "TRENDING", "RISING"])
    )
    
    # SÃ©lectionner et ordonner les colonnes finales
    final_df = trending_only.select(
        col("window.start").alias("window_start"),
        col("window.end").alias("window_end"),
        col("product_id"),
        col("product_type"),
        col("category"),
        col("views_count"),
        col("unique_viewers"),
        col("engagement_rate"),
        col("trend_status"),
        col("trend_badge"),
        col("alert_message"),
        col("detected_at")
    )
    
    return final_df

def write_to_mongodb(df):
    """
    Configure l'Ã©criture vers MongoDB
    
    Args:
        df: DataFrame Ã  Ã©crire
    
    Returns:
        StreamingQuery
    """
    print("\nğŸ’¾ Configuration de l'Ã©criture vers MongoDB...")
    
    query = df.writeStream \
        .outputMode("append") \
        .format("mongo") \
        .option("database", MONGO_DB) \
        .option("collection", MONGO_COLLECTION) \
        .option("checkpointLocation", "/tmp/spark-checkpoint-trending") \
        .start()
    
    return query

def write_to_console(df):
    """
    Configuration de l'affichage console (pour debugging)
    
    Args:
        df: DataFrame Ã  afficher
    
    Returns:
        StreamingQuery
    """
    print("\nğŸ“º Configuration de l'affichage console...")
    
    query = df.writeStream \
        .outputMode("append") \
        .format("console") \
        .option("truncate", "false") \
        .start()
    
    return query

def main():
    """Fonction principale"""
    # CrÃ©er la session Spark
    spark = create_spark_session()
    spark.sparkContext.setLogLevel("WARN")
    
    try:
        # Setup streaming
        trending_df = setup_streaming_query(spark)
        
        # Ã‰criture simultanÃ©e vers MongoDB et console
        mongodb_query = write_to_mongodb(trending_df)
        console_query = write_to_console(trending_df)
        
        print("\n" + "="*60)
        print("âœ… STREAMING DÃ‰MARRÃ‰ - DÃ‰TECTION DES TENDANCES...")
        print("="*60)
        print("\nğŸ’¡ Le systÃ¨me surveille maintenant les produits populaires")
        print("   Appuyez sur Ctrl+C pour arrÃªter\n")
        
        # Attendre la terminaison
        mongodb_query.awaitTermination()
        console_query.awaitTermination()
        
    except KeyboardInterrupt:
        print("\n\nğŸ›‘ ArrÃªt demandÃ© par l'utilisateur...")
        spark.streams.active[0].stop()
        print("âœ… Streaming arrÃªtÃ© proprement\n")
    except Exception as e:
        print(f"\nâŒ ERREUR: {str(e)}")
        import traceback
        traceback.print_exc()
    finally:
        spark.stop()

if __name__ == "__main__":
    main()
