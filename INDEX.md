# üìö INDEX DU PROJET - Navigation Rapide

## üéØ Par O√π Commencer ?

### üöÄ Vous voulez d√©marrer rapidement ?
üëâ **[QUICKSTART.md](QUICKSTART.md)** - D√©marrage en 10 minutes

### üìñ Vous voulez comprendre l'architecture ?
üëâ **[ARCHITECTURE.md](ARCHITECTURE.md)** - Justifications techniques compl√®tes

### üìã Vous voulez la documentation compl√®te ?
üëâ **[README.md](README.md)** - Guide complet du projet

### üì¶ Vous pr√©parez le rendu ?
üëâ **[LIVRABLE.md](LIVRABLE.md)** - Document de livraison acad√©mique

---

## üìÅ Structure du Projet

```
bigdata-logs-analysis/
‚îÇ
‚îú‚îÄ‚îÄ üìÑ README.md                    # Documentation principale
‚îú‚îÄ‚îÄ üìÑ ARCHITECTURE.md              # Justifications techniques
‚îú‚îÄ‚îÄ üìÑ QUICKSTART.md                # Guide d√©marrage rapide
‚îú‚îÄ‚îÄ üìÑ LIVRABLE.md                  # Document de rendu
‚îú‚îÄ‚îÄ üìÑ INDEX.md                     # Ce fichier
‚îÇ
‚îú‚îÄ‚îÄ üê≥ docker-compose.yml           # Orchestration des services
‚îú‚îÄ‚îÄ üìù .gitignore                   # Fichiers √† ignorer
‚îÇ
‚îú‚îÄ‚îÄ üìÇ data/                        # Donn√©es
‚îÇ   ‚îú‚îÄ‚îÄ generate_logs.py           # G√©n√©rateur de logs
‚îÇ   ‚îî‚îÄ‚îÄ web_server.log             # Logs g√©n√©r√©s (10k lignes)
‚îÇ
‚îú‚îÄ‚îÄ üìÇ spark/                       # Applications Spark
‚îÇ   ‚îú‚îÄ‚îÄ requirements.txt           # D√©pendances Python
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ üìÇ batch/                  # Analyses Batch
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ top_products.py        # Analyse #1: Top produits
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ http_codes.py          # Analyse #2: Codes HTTP
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ top_ips.py             # Analyse #3: Top IPs
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ üìÇ streaming/              # Analyses Streaming
‚îÇ       ‚îú‚îÄ‚îÄ error_detection.py     # Streaming #1: Erreurs
‚îÇ       ‚îî‚îÄ‚îÄ trending_products.py   # Streaming #2: Tendances
‚îÇ
‚îú‚îÄ‚îÄ üìÇ kafka/                       # Kafka
‚îÇ   ‚îú‚îÄ‚îÄ requirements.txt           # D√©pendances Python
‚îÇ   ‚îî‚îÄ‚îÄ log_producer.py            # Producteur Kafka
‚îÇ
‚îî‚îÄ‚îÄ üìÇ scripts/                     # Scripts utilitaires
    ‚îú‚îÄ‚îÄ setup.sh                   # Configuration initiale
    ‚îú‚îÄ‚îÄ prepare_hdfs.sh            # Pr√©paration HDFS
    ‚îú‚îÄ‚îÄ run_batch.sh               # Lancer analyses batch
    ‚îú‚îÄ‚îÄ run_streaming.sh           # Guide streaming
    ‚îú‚îÄ‚îÄ stop.sh                    # Arr√™ter les services
    ‚îî‚îÄ‚îÄ clean.sh                   # Nettoyage complet
```

---

## üéì Parcours d'Apprentissage Recommand√©

### Niveau 1: D√©butant
1. Lire **[QUICKSTART.md](QUICKSTART.md)**
2. Ex√©cuter `./scripts/setup.sh`
3. Lancer `docker-compose up -d`
4. Voir les r√©sultats dans MongoDB

**Temps estim√©**: 15 minutes

---

### Niveau 2: Interm√©diaire
1. Comprendre l'architecture dans **[ARCHITECTURE.md](ARCHITECTURE.md)**
2. √âtudier le code des analyses batch
3. Modifier les seuils et relancer
4. Explorer les interfaces web

**Temps estim√©**: 1 heure

---

### Niveau 3: Avanc√©
1. Lire la section "M√©thode de Raisonnement"
2. Tester le streaming en conditions r√©elles
3. Impl√©menter une nouvelle analyse
4. Optimiser les performances Spark

**Temps estim√©**: 2-3 heures

---

## üîç Trouver une Information

### Je cherche...

#### ...comment d√©marrer le projet
‚Üí [QUICKSTART.md](QUICKSTART.md) - Section "Installation en 5 √âtapes"

#### ...les justifications techniques
‚Üí [ARCHITECTURE.md](ARCHITECTURE.md) - Section "Justifications Techniques"

#### ...comment ex√©cuter les analyses batch
‚Üí [README.md](README.md) - Section "Ex√©cution des Analyses"

#### ...comment tester le streaming
‚Üí [QUICKSTART.md](QUICKSTART.md) - Section "Tester le Streaming"

#### ...le format des donn√©es en entr√©e
‚Üí [README.md](README.md) - Section "Description du Dataset"

#### ...les r√©sultats MongoDB
‚Üí [README.md](README.md) - Section "Consultation des R√©sultats"

#### ...les interfaces web disponibles
‚Üí [QUICKSTART.md](QUICKSTART.md) - Section "Interfaces Web Disponibles"

#### ...comment d√©panner une erreur
‚Üí [QUICKSTART.md](QUICKSTART.md) - Section "D√©pannage Rapide"

#### ...les technologies utilis√©es
‚Üí [LIVRABLE.md](LIVRABLE.md) - Section "Architecture Technique"

#### ...les comp√©tences d√©montr√©es
‚Üí [LIVRABLE.md](LIVRABLE.md) - Section "Comp√©tences D√©montr√©es"

---

## üìä Analyses Disponibles

### Batch (Donn√©es Historiques)

| # | Nom | Fichier | Objectif |
|---|-----|---------|----------|
| 1 | Top Produits | `spark/batch/top_products.py` | Identifier les 10 produits les plus consult√©s |
| 2 | Codes HTTP | `spark/batch/http_codes.py` | KPIs de sant√© du serveur (succ√®s, erreurs) |
| 3 | Top IPs | `spark/batch/top_ips.py` | D√©tecter utilisateurs actifs et bots |

### Streaming (Temps R√©el)

| # | Nom | Fichier | Objectif |
|---|-----|---------|----------|
| 1 | D√©tection Erreurs | `spark/streaming/error_detection.py` | Alertes sur pics d'erreurs 404/500 |
| 2 | Produits Tendance | `spark/streaming/trending_products.py` | Identifier produits populaires (>20 vues/min) |

---

## üõ†Ô∏è Technologies & Ports

| Service | Technologie | Version | Port(s) | Interface Web |
|---------|------------|---------|---------|---------------|
| HDFS NameNode | Hadoop | 3.2.1 | 9870, 9000 | ‚úÖ http://localhost:9870 |
| HDFS DataNode | Hadoop | 3.2.1 | 9864 | ‚úÖ http://localhost:9864 |
| Spark Master | Spark | 3.3.0 | 8080, 7077 | ‚úÖ http://localhost:8080 |
| Spark Worker | Spark | 3.3.0 | 8081 | ‚úÖ http://localhost:8081 |
| Kafka | Kafka | 7.3.0 | 9092, 9093 | ‚ùå CLI uniquement |
| Zookeeper | Zookeeper | 7.3.0 | 2181 | ‚ùå CLI uniquement |
| MongoDB | MongoDB | 6.0 | 27017 | ‚ùå CLI uniquement |

---

## üìù Commandes Essentielles

### D√©marrage
```bash
./scripts/setup.sh              # Configuration initiale
docker-compose up -d            # D√©marrer les services
./scripts/prepare_hdfs.sh       # Pr√©parer HDFS
./scripts/run_batch.sh          # Lancer analyses batch
```

### V√©rification
```bash
docker-compose ps               # √âtat des conteneurs
docker logs -f spark-master     # Logs Spark
docker exec -it mongodb mongo   # Shell MongoDB
```

### Arr√™t
```bash
./scripts/stop.sh               # Arr√™ter proprement
./scripts/clean.sh              # Nettoyage complet
```

---

## üéØ Checklist de Validation

Avant de rendre le projet, v√©rifiez:

- [ ] Tous les conteneurs sont "Up" (`docker-compose ps`)
- [ ] HDFS contient les logs (`docker exec namenode hdfs dfs -ls /logs`)
- [ ] Les 3 analyses batch s'ex√©cutent sans erreur
- [ ] Les r√©sultats sont dans MongoDB (`db.top_products.find()`)
- [ ] Les interfaces web sont accessibles
- [ ] La documentation est compl√®te (4 fichiers MD)
- [ ] Le code est comment√© et propre
- [ ] Les scripts sont ex√©cutables (`chmod +x`)

---

## üìö Ressources Compl√©mentaires

### Documentation Officielle
- [Apache Spark](https://spark.apache.org/docs/latest/) - Documentation Spark
- [Apache Kafka](https://kafka.apache.org/documentation/) - Documentation Kafka
- [HDFS](https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HdfsUserGuide.html) - Guide HDFS
- [MongoDB](https://docs.mongodb.com/) - Documentation MongoDB

### Tutoriels
- [Spark RDD Programming Guide](https://spark.apache.org/docs/latest/rdd-programming-guide.html)
- [Structured Streaming Guide](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html)
- [Kafka Quickstart](https://kafka.apache.org/quickstart)

---

## üí° Astuces

### Performance
- Augmenter `SPARK_WORKER_MEMORY` pour gros volumes
- Ajuster `spark.sql.shuffle.partitions` pour optimiser les shuffles
- Utiliser `.cache()` sur les DataFrames r√©utilis√©s

### Debugging
- Activer les logs d√©taill√©s: `spark.sparkContext.setLogLevel("INFO")`
- V√©rifier les jobs dans Spark UI: http://localhost:4040
- Monitorer HDFS dans NameNode UI: http://localhost:9870

### Production
- Augmenter le facteur de r√©plication HDFS √† 3
- Ajouter des Workers Spark pour parall√©lisme
- Impl√©menter des checkpoints pour fault-tolerance
- Mettre en place un monitoring (Prometheus/Grafana)

---

## ü§ù Contribution

Ce projet est acad√©mique. Pour des am√©liorations:
1. Forker le projet
2. Cr√©er une branche (`git checkout -b feature/amelioration`)
3. Commiter les changements (`git commit -m 'Ajout fonctionnalit√©'`)
4. Pusher (`git push origin feature/amelioration`)
5. Cr√©er une Pull Request

---

## üìû Support

Probl√®me rencontr√© ?
1. Consulter [QUICKSTART.md](QUICKSTART.md) - Section "D√©pannage"
2. V√©rifier les logs: `docker logs -f <service>`
3. Red√©marrer: `docker-compose restart <service>`
4. Nettoyage complet: `./scripts/clean.sh` puis recommencer

---

## üéâ F√©licitations !

Vous avez maintenant acc√®s √† une architecture Big Data compl√®te et fonctionnelle.

**Bon courage pour votre projet ! üöÄ**

---

*Document g√©n√©r√© le 28 Janvier 2025*  
*Projet acad√©mique - Architecture Big Data Distribu√©e*
