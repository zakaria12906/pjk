# TP AvancÃ© - Analyse de Logs Web avec Architecture Big Data

## Description

Analyse de logs web d'un site e-commerce de cosmÃ©tiques utilisant une architecture Big Data distribuÃ©e (HDFS, Spark, Kafka, MongoDB).

---

## ğŸ“ Structure du Projet

```
Projet_charazad/
â”œâ”€â”€ docker-compose.yml              # Orchestration des services
â”œâ”€â”€ data/
â”‚   â””â”€â”€ web_server.log             # Fichier de logs Ã  analyser
â”œâ”€â”€ spark/
â”‚   â”œâ”€â”€ requirements.txt           # pyspark, pymongo
â”‚   â”œâ”€â”€ batch/
â”‚   â”‚   â””â”€â”€ top_products.py        # Analyse batch: Top 10 produits
â”‚   â””â”€â”€ streaming/
â”‚       â””â”€â”€ error_detection.py     # Analyse streaming: DÃ©tection erreurs
â””â”€â”€ kafka/
    â”œâ”€â”€ requirements.txt           # kafka-python
    â””â”€â”€ log_producer.py            # Producteur Kafka
```

---

## ğŸ¯ Analyses ImplÃ©mentÃ©es

### 1. Analyse Batch - Top 10 Produits

- **Fichier**: `spark/batch/top_products.py`
- **Objectif**: Identifier les produits les plus consultÃ©s
- **Source**: HDFS (`/logs/web_server.log`)
- **Output**: MongoDB (`logs_analytics.top_products`)

### 2. Analyse Streaming - DÃ©tection d'Erreurs

- **Fichier**: `spark/streaming/error_detection.py`
- **Objectif**: DÃ©tecter pics d'erreurs 404/500 sur fenÃªtre 5 minutes
- **Source**: Kafka (topic `web-logs`)
- **Output**: MongoDB (`logs_analytics.error_alerts`)

---

## ğŸ—ï¸ Architecture

7 services Docker orchestrÃ©s:

| Service | Port | RÃ´le |
|---------|------|------|
| namenode | 9870, 9000 | HDFS NameNode |
| datanode | 9864 | HDFS DataNode |
| spark-master | 8080, 7077 | Spark Master |
| spark-worker | 8081 | Spark Worker |
| zookeeper | 2181 | Coordination |
| kafka | 9092 | Message Broker |
| mongodb | 27017 | Base de donnÃ©es |

---

## ğŸš€ Installation et ExÃ©cution

### 1. DÃ©marrer les services

```bash
docker-compose up -d
sleep 120  # Attendre 2 minutes
```

### 2. PrÃ©parer HDFS

```bash
docker exec namenode hdfs dfs -mkdir -p /logs
docker exec namenode hdfs dfs -chmod -R 777 /logs
docker exec namenode hdfs dfs -put /data/web_server.log /logs/
```

### 3. ExÃ©cuter l'analyse Batch

```bash
docker exec spark-master spark-submit \
  --master spark://spark-master:7077 \
  --packages org.mongodb.spark:mongo-spark-connector_2.12:3.0.1 \
  /spark-apps/batch/top_products.py
```

**RÃ©sultats**:
```bash
docker exec -it mongodb mongo
> use logs_analytics
> db.top_products.find().pretty()
```

### 4. ExÃ©cuter l'analyse Streaming

**Terminal 1 - Producteur**:
```bash
docker exec -it kafka bash
cd /kafka-apps
pip3 install -r requirements.txt
python3 log_producer.py
# Choisir: 2 (ERRORS), DurÃ©e: 300
```

**Terminal 2 - Spark Streaming**:
```bash
docker exec spark-master spark-submit \
  --master spark://spark-master:7077 \
  --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0,org.mongodb.spark:mongo-spark-connector_2.12:3.0.1 \
  /spark-apps/streaming/error_detection.py
```

**RÃ©sultats** (aprÃ¨s 5-6 minutes):
```bash
docker exec -it mongodb mongo
> use logs_analytics
> db.error_alerts.find().sort({detected_at: -1}).pretty()
```

---

## ğŸ“Š Interfaces Web

- **HDFS**: http://localhost:9870
- **Spark Master**: http://localhost:8080
- **Spark Worker**: http://localhost:8081

---

## ğŸ›‘ ArrÃªter

```bash
docker-compose down
```

Pour supprimer les volumes (donnÃ©es):
```bash
docker-compose down -v
```

---

## ğŸ› ï¸ Technologies

- HDFS 3.2.1 - Stockage distribuÃ©
- Apache Spark 3.3.0 - Traitement batch et streaming
- Apache Kafka 7.3.0 - Streaming de donnÃ©es
- MongoDB 6.0 - Stockage des rÃ©sultats
- Docker & Docker Compose - Orchestration

---

## ğŸ› DÃ©pannage

### Port dÃ©jÃ  utilisÃ©
```bash
lsof -i :9870
kill -9 <PID>
```

### HDFS en safe mode
```bash
docker exec namenode hdfs dfsadmin -safemode leave
```

### Voir les logs
```bash
docker logs -f spark-master
docker logs -f kafka
```

---

## ğŸ“¦ DÃ©pÃ´t GitHub

https://github.com/zakaria12906/pjk.git
