# üöÄ COMMANDES DE TEST - VERSION CORRIG√âE

## ‚ö†Ô∏è PROBL√àME R√âSOLU

Les images Spark avec versions sp√©cifiques n'existent pas.
**Solution**: Utilisation de `bitnami/spark:latest`

---

## ‚úÖ √âTAPES COMPL√àTES

### √âTAPE 1 - Mise √† jour et nettoyage

```bash
cd /Users/zakariaeelouazzani/Desktop/Projet_charazad

# R√©cup√©rer la version corrig√©e
git pull origin main

# Nettoyer les tentatives pr√©c√©dentes
docker-compose down -v
docker system prune -f
```

---

### √âTAPE 2 - D√©marrer les services

```bash
# D√©marrer tous les services (t√©l√©chargement ~5-10 min la 1√®re fois)
docker-compose up -d
```

**‚è≥ Attendez que toutes les images soient t√©l√©charg√©es...**

---

### √âTAPE 3 - Attendre le d√©marrage complet

```bash
# Attendre 2 minutes
echo "‚è≥ Attente 2 minutes..."
sleep 120
```

---

### √âTAPE 4 - V√©rifier l'√©tat

```bash
docker-compose ps
```

**R√©sultat attendu**: Tous affichent "Up" ou "running"

---

### √âTAPE 5 - V√©rifier les interfaces web

Ouvrir dans le navigateur:
- HDFS: http://localhost:9870
- Spark Master: http://localhost:8080
- Spark Worker: http://localhost:8081

---

### √âTAPE 6 - Pr√©parer HDFS

```bash
# Cr√©er le r√©pertoire
docker exec namenode hdfs dfs -mkdir -p /logs
docker exec namenode hdfs dfs -chmod -R 777 /logs

# Copier les logs
docker exec namenode hdfs dfs -put /data/web_server.log /logs/

# V√©rifier
docker exec namenode hdfs dfs -ls /logs
docker exec namenode hdfs dfs -cat /logs/web_server.log | head -10
```

---

### √âTAPE 7 - Test BATCH

```bash
docker exec spark-master spark-submit \
  --master spark://spark-master:7077 \
  --packages org.mongodb.spark:mongo-spark-connector_2.12:10.0.5 \
  /spark-apps/batch/top_products.py
```

**V√©rifier les r√©sultats**:
```bash
docker exec -it mongodb mongosh
```

Dans mongosh:
```javascript
use logs_analytics
db.top_products.find()
db.top_products.countDocuments()
exit
```

---

### √âTAPE 8 - Test STREAMING

**Terminal 1 - Producteur**:
```bash
docker exec -it kafka bash
cd /kafka-apps
pip3 install -r requirements.txt
python3 log_producer.py
# Choisir: 2 (ERRORS)
# Dur√©e: 300
```

**Terminal 2 - Spark Streaming**:
```bash
docker exec spark-master spark-submit \
  --master spark://spark-master:7077 \
  --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0,org.mongodb.spark:mongo-spark-connector_2.12:10.0.5 \
  /spark-apps/streaming/error_detection.py
```

**Terminal 3 - R√©sultats** (apr√®s 5-6 minutes):
```bash
docker exec -it mongodb mongosh
use logs_analytics
db.error_alerts.find().sort({detected_at: -1}).limit(5)
exit
```

---

### √âTAPE 9 - Arr√™ter

```bash
docker-compose down
```

---

## üêõ SI PROBL√àME

### Voir les logs
```bash
docker-compose logs -f
```

### Red√©marrer un service
```bash
docker-compose restart spark-master
```

### Tout nettoyer et recommencer
```bash
docker-compose down -v
docker system prune -af
docker-compose up -d
```
